{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "V-76o0n56zEg",
    "outputId": "93ee6114-8d8f-4d1e-b339-7e35370522c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFbcdH9J7Wsc"
   },
   "source": [
    "#Augmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8u4x7M3X7Vfm"
   },
   "outputs": [],
   "source": [
    "#THIS BLOCK IS IMPORTANT\n",
    "\n",
    "\n",
    "def importClasses():\n",
    "    classPath = \"./tiny-imagenet-200/wnids.txt\"\n",
    "    return open(classPath).read().splitlines()\n",
    "\n",
    "trainClasses = importClasses()\n",
    "\n",
    "def createDict(trainClasses):\n",
    "    #Mapping from class name to class Number\n",
    "    classDict = {}\n",
    "    for i in range(len(trainClasses)):\n",
    "        classDict[trainClasses[i]] = i\n",
    "    return classDict\n",
    "\n",
    "classNumDict = createDict(trainClasses)\n",
    "\n",
    "def getMiddlePatch(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "    except IOError:\n",
    "        return\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.asarray(img)\n",
    "    return img[3:59, 3:59, :]\n",
    "    \n",
    "\n",
    "def pcaAug(original_image):\n",
    "    #Takes in an image as a np array and returns the pca augmented image as a np array\n",
    "    renorm_image = np.reshape(original_image,(original_image.shape[0]*original_image.shape[1],3))\n",
    "\n",
    "    renorm_image = renorm_image.astype('float32')\n",
    "    renorm_image -= np.mean(renorm_image, axis=0)\n",
    "    renorm_image /= np.std(renorm_image, axis=0)\n",
    "\n",
    "    cov = np.cov(renorm_image, rowvar=False)\n",
    "\n",
    "    lambdas, p = np.linalg.eig(cov)\n",
    "    alphas = np.random.normal(0, 0.1, 3)\n",
    "\n",
    "    delta = np.dot(p, alphas*lambdas)\n",
    "\n",
    "    delta = (delta).astype('int8')\n",
    "\n",
    "    pca_color_image = np.maximum(np.minimum(original_image + delta, 255), 0).astype('uint8')\n",
    "    return pca_color_image\n",
    "\n",
    "\n",
    "def transToOneHot(numClass, dims = 200):\n",
    "    vec = np.zeros(dims)\n",
    "    vec[numClass] = 1\n",
    "    return vec\n",
    "\n",
    "def generateImgs(path, outClassNum):\n",
    "    #Takes in path of Image and generates 5 patches and their mirror images and returns them as a list of np arrays and an array of one hot encoded output\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "    except IOError:\n",
    "        return\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.asarray(img)\n",
    "    arr = [img[0:56, 0:56, :], img[8:, 0:56, :], img[0:56, 8:, :], img[8:, 8:, :], img[3:59, 3:59, :]]\n",
    "    arr = arr + [np.flip(arr[0], 1),np.flip(arr[1], 1), np.flip(arr[2], 1), np.flip(arr[3], 1), np.flip(arr[4], 1)]\n",
    "    pcaArr = []\n",
    "    for i in range(len(arr)):\n",
    "        pcaArr.append(pcaAug(arr[i]))\n",
    "    inputArr = arr + pcaArr\n",
    "    oneHot = transToOneHot(outClassNum, dims = 200)\n",
    "    outArr = []\n",
    "    for i in range(len(inputArr)):\n",
    "        outArr.append(oneHot)\n",
    "    return inputArr, outArr\n",
    "\n",
    "def getTrainingSample():\n",
    "    #Returns one batch of training samples using any 5 random images. Returns a batch of 100\n",
    "    inputArray = []\n",
    "    outputArray = []\n",
    "    for i in range(5):\n",
    "        outClassNum = np.random.randint(len(trainClasses))\n",
    "        path = \"./tiny-imagenet-200/train/\" + str(trainClasses[outClassNum]) + \"/images/\" + str(trainClasses[outClassNum]) + \"_\" + str(np.random.randint(100)) + \".JPEG\"\n",
    "        inArr, outArr = generateImgs(path, outClassNum)\n",
    "        inputArray = inputArray + inArr\n",
    "        outputArray = outputArray + outArr\n",
    "    #temp = np.array(100,3)\n",
    "    #temp[:, 0] = inputArray\n",
    "    #temp[:,1] = outputArray\n",
    "    #temp[:,2] = range(100)\n",
    "    #np.random.shuffle(temp)\n",
    "    #inputArray = temp[:,0]\n",
    "    #outputArray = temp[:,1]\n",
    "    return inputArray, outputArray\n",
    "\n",
    "def getOutputArray():\n",
    "    outputArray = []\n",
    "    outputTextPath = \"./tiny-imagenet-200/val/val_annotations.txt\"\n",
    "    textFile = open(outputTextPath).read().split()\n",
    "    for i in range(1000):\n",
    "        outputArray.append(transToOneHot(classNumDict[textFile[6*i+1]], dims = 200))\n",
    "    return outputArray\n",
    "\n",
    "def getTestSample():\n",
    "    inputArray = []\n",
    "    outputArray = getOutputArray()\n",
    "    \n",
    "    \n",
    "    for i in range(1000):\n",
    "        path = \"./tiny-imagenet-200/val/images/val_\" + str(i) + \".JPEG\"\n",
    "        img = getMiddlePatch(path)\n",
    "        inputArray.append(img)\n",
    "        \n",
    "    return inputArray, outputArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#imagenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRSCtQgU7u1a"
   },
   "outputs": [],
   "source": [
    "image_size = 56 #We should use 56 instead, using 64x64 images\n",
    "\n",
    "input_images = tf.placeholder(tf.float32, shape= [None, image_size, image_size,3], name = \"input_images\")\n",
    "kernel = tf.Variable(tf.truncated_normal([11,11,3,96], dtype=tf.float32, stddev=1e-2), name=\"conv1_weights\")\n",
    "\n",
    "conv = tf.nn.conv2d(input_images, kernel, [1,4,4,1], padding=\"SAME\") #[1,4,4,1] === [1,stride,stride,1] see documentation\n",
    "bias = tf.Variable(tf.truncated_normal([96]), name=\"conv1_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv1 = tf.nn.relu(conv_with_bias, name=\"conv1\")\n",
    "lrn1 = tf.nn.lrn(conv1, alpha = 1e-4, beta=0.75, depth_radius=5, bias=2.0) #LRN = Local Response Normalisation..Depth Radius is most probably n, need to verify\n",
    "pooled_conv1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84KGW9Rw8AFA"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([5,5,96,256], dtype=tf.float32, stddev=1e-2), name=\"conv2_weights\")\n",
    "conv = tf.nn.conv2d(pooled_conv1, kernel, [1, 4, 4, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([256]), name=\"conv2_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv2 = tf.nn.relu(conv_with_bias, name=\"conv2\")\n",
    "lrn2 = tf.nn.lrn(conv2,\n",
    "                 alpha=1e-4,\n",
    "                 beta=0.75,\n",
    "                 depth_radius=5,\n",
    "                 bias=2.0)\n",
    "\n",
    "pooled_conv2 = tf.nn.max_pool(lrn2,\n",
    "                              ksize=[1, 3, 3, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",\n",
    "                              name=\"pool2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K6HERaR8CpL"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv3_weights\")\n",
    "conv = tf.nn.conv2d(pooled_conv2, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.truncated_normal([384]), name=\"conv3_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv3 = tf.nn.relu(conv_with_bias, name=\"conv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfam5tmu8FVe"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv4_weights\")\n",
    "conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([384]), name=\"conv4_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv4 = tf.nn.relu(conv_with_bias, name=\"conv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gArLGIGV8NHQ"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv5_weights\")\n",
    "conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([256]), name=\"conv5_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv5 = tf.nn.relu(conv_with_bias, name=\"conv5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "ARmWuSum8N0m",
    "outputId": "b22fff7f-bc61-4c15-e874-9ea9e18980de"
   },
   "outputs": [],
   "source": [
    "fc_size = 256\n",
    "conv5 = tf.layers.flatten(conv5)\n",
    "weights = tf.Variable(tf.truncated_normal([fc_size, fc_size]), name = \"fc1_weights\")\n",
    "bias = tf.Variable(tf.ones([fc_size]), name=\"fc1_bias\")\n",
    "fc1 = tf.matmul(conv5, weights) + bias\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob = 0.5) #Check this line, might cause problem\n",
    "fc1 = tf.nn.relu(fc1, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufUIpZaO8RLj"
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([fc_size, fc_size]), name=\"fc2_weights\")\n",
    "bias = tf.Variable(tf.ones([fc_size]), name=\"fc2_bias\")\n",
    "fc2 = tf.matmul(fc1, weights) + bias\n",
    "fc2 = tf.nn.dropout(fc2, keep_prob = 0.5) #Check this line, might cause problem\n",
    "fc2 = tf.nn.relu(fc2, name=\"fc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-duKoBWc8WeP"
   },
   "outputs": [],
   "source": [
    "n_classes = 200\n",
    "weights = tf.Variable(tf.zeros([fc_size, n_classes]), name=\"output_weights\")\n",
    "bias = tf.Variable(tf.ones([n_classes]), name=\"output_bias\")\n",
    "out = tf.matmul(fc2, weights) + bias\n",
    "out = tf.nn.softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kk0SpxD18Z8O"
   },
   "source": [
    "#training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "-JmP27Vp8bpf",
    "outputId": "94cafb20-05c3-43b2-f80d-be29fa7c0eb3"
   },
   "outputs": [],
   "source": [
    "# all the parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "no_of_epochs = 5\n",
    "total_batches = 5\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "# cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = out))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# optimizer = tfa.optimizers.weight_decay_optimizers.SGDW(\n",
    "#                 learning_rate=learning_rate, \n",
    "#                 momentum=momentum, \n",
    "#                 weight_decay=weight_decay, \n",
    "#                 nesterov=True, name='SGDW').minimize(cost, var_list = var_list)\n",
    "\n",
    "\n",
    "# accuracy functions\n",
    "top_1 = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "top_1_accuracy = tf.reduce_mean(tf.cast(top_1, tf.float32))\n",
    "top_5 = tf.math.in_top_k(out, tf.argmax(y, 1), 5)\n",
    "top_5_accuracy = tf.reduce_mean(tf.cast(top_5, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # for plotting\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy_top_1 = []\n",
    "    train_accuracy_top_5 = []\n",
    "    test_accuracy_top_1 = []\n",
    "    test_accuracy_top_5 = []\n",
    "\n",
    "    inp_test, out_test = getTestSample() # so that after every epoch we test the same dataset\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "\n",
    "    for epoch in range(no_of_epochs):\n",
    "        # running the architecture for 1 epoc\n",
    "        for batch_no in range(total_batches):\n",
    "            inp_train, out_train = getTrainingSample()\n",
    "            sess.run([optimizer],\n",
    "                        feed_dict={\n",
    "                            input_images: inp_train,\n",
    "                            y: out_train})\n",
    "        \n",
    "        # calculating accuracy and loss after 1 epoc for the last training data\n",
    "        top_1_acc, top_5_acc, loss = sess.run([top_1_accuracy, top_5_accuracy, cost],\n",
    "                                                                feed_dict={\n",
    "                                                                    input_images: inp_train,\n",
    "                                                                    y: out_train})\n",
    "        print(\"Epoch: {}, Top 1 Acc (training): {}, Top 5 Acc (training) = {}, Loss: {} \".format(epoch, top_1_acc, top_5_acc, loss))\n",
    "            \n",
    "        # calculating accuracy and loss after 1 epoc for the test data\n",
    "        test_top_1_acc, test_top_5_acc, valid_loss = sess.run([top_1_accuracy, top_5_accuracy, cost],\n",
    "                                                                feed_dict={\n",
    "                                                                    input_images: inp_test,\n",
    "                                                                    y: out_test})\n",
    "        print(\"Top 1 Acc (testing): {}, Top 5 Acc (testing) = {}, Validation Loss: {}\".format(test_top_1_acc, test_top_5_acc, valid_loss))\n",
    "\n",
    "        # storing for plotting\n",
    "        train_loss.append(loss)\n",
    "        test_loss.append(valid_loss)\n",
    "        train_accuracy_top_1.append(top_1_acc)\n",
    "        train_accuracy_top_5.append(top_5_acc)\n",
    "        test_accuracy_top_1.append(test_top_1_acc)\n",
    "        test_accuracy_top_5.append(test_top_5_acc)\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "    # plotting training and testing - loss\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
    "    plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
    "    plt.title('Training and Test loss')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "    # plotting training and testing - top 1 accuracy\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(len(train_loss)), train_accuracy_top_1, 'b', label='Top 1 Training Accuracy')\n",
    "    plt.plot(range(len(train_loss)), test_accuracy_top_1, 'r', label='Top 1 Test Accuracy')\n",
    "    plt.title('Top 1 Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "    # plotting training and testing - top 5 accuracy\n",
    "    plt.figure(2)\n",
    "    plt.plot(range(len(train_loss)), train_accuracy_top_5, 'b', label='Top 5 Training Accuracy')\n",
    "    plt.plot(range(len(train_loss)), test_accuracy_top_5, 'r', label='Top 5 Test Accuracy')\n",
    "    plt.title('Top 5 Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miUGr7rfQgWT"
   },
   "outputs": [],
   "source": [
    "# Delete Everything after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "alexnet-nnfl.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
