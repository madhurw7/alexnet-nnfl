{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKmoZqGATEBJ"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eB1SymK7TLsP",
    "outputId": "c5e98ae9-a233-4e28-c737-62f69dafb29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Akc4NI0_TNen",
    "outputId": "299cf190-36f7-47a7-9a7b-a40786e09e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/nnfl-alexnet-data\n"
     ]
    }
   ],
   "source": [
    "# %cd drive/'My Drive'/nnfl-alexnet-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SjWXVz7fTPNe",
    "outputId": "e74c3289-ed83-4342-8e2c-e08f6cb629bb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f4157790e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#helper functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./valDataComplete.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minputVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#helper functions\n",
    "\n",
    "valData = np.load('./valDataComplete.npz')\n",
    "inputVal = valData['arr_0']\n",
    "outputVal = valData['arr_1']\n",
    "\n",
    "trainData = np.load('./trainDataComplete.npz')\n",
    "inputTrain = trainData['arr_0']\n",
    "outputTrain = trainData['arr_1']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pcaAug(original_image):\n",
    "    #Takes in an image as a np array and returns the pca augmented image as a np array\n",
    "    renorm_image = np.reshape(original_image,(original_image.shape[0]*original_image.shape[1],3))\n",
    "\n",
    "    renorm_image = renorm_image.astype('float32')\n",
    "    renorm_image -= np.mean(renorm_image, axis=0)\n",
    "    renorm_image /= np.std(renorm_image, axis=0)\n",
    "\n",
    "    cov = np.cov(renorm_image, rowvar=False)\n",
    "\n",
    "    lambdas, p = np.linalg.eig(cov)\n",
    "    alphas = np.random.normal(0, 0.1, 3)\n",
    "\n",
    "    delta = np.dot(p, alphas*lambdas)\n",
    "\n",
    "    delta = (delta).astype('int8')\n",
    "\n",
    "    pca_color_image = np.maximum(np.minimum(original_image + delta, 255), 0).astype('uint8')\n",
    "    return pca_color_image\n",
    "\n",
    "\n",
    "def randomizeOrder(inArr, outArr, n):\n",
    "    #Randomizes order of input Array and does the same for outputArray keeping relative positions same\n",
    "    idx = np.array(range(n))\n",
    "    np.random.shuffle(idx)\n",
    "    inList = []\n",
    "    outList = []\n",
    "    for i in range(n):\n",
    "        inList.append(inArr[idx[i]])\n",
    "        outList.append(outArr[idx[i]])\n",
    "    return inList, outList\n",
    "\n",
    "\n",
    "inMean = 112.69858580973307\n",
    "inStd = 70.93752549462194\n",
    "#Constants calculated on the entire training dataset\n",
    "\n",
    "\n",
    "def applyAug(img, outputOneHot):\n",
    "    #Takes in an np array image and onHotEncoded output class and returns normalised and augmented images.\n",
    "    #Normalises Image using the constants defined above\n",
    "    #Takes five patches of the image, flips all of them, and applies PCA, creates 20 samples from 1 image\n",
    "    #Output Image size is 56x56x3 from 64x64x3, the ratio is kept to be same as mentioned in the paper\n",
    "    img = (img - inMean)/inStd\n",
    "    arr = [img[0:56, 0:56, :], img[7:, 0:56, :], img[0:56, 7:, :], img[7:, 7:, :], img[3:59, 3:59, :]]\n",
    "    arr = arr + [np.flip(arr[0], 1),np.flip(arr[1], 1), np.flip(arr[2], 1), np.flip(arr[3], 1), np.flip(arr[4], 1)]\n",
    "    pcaArr = []\n",
    "    for i in range(len(arr)):\n",
    "        pcaArr.append(pcaAug(arr[i]))\n",
    "    inputArr = arr + pcaArr\n",
    "    outArr = []\n",
    "    for i in range(len(inputArr)):\n",
    "        outArr.append(outputOneHot)\n",
    "    return inputArr, outArr  \n",
    "\n",
    "\n",
    "def createTrainingSet():\n",
    "    #Passes the entire data set to Augmentation function, structures the entire augmented data and \n",
    "    #randomizes the order using the randomizeOrder function\n",
    "    inputArray = []\n",
    "    outputArray = []\n",
    "    for i in range(len(inputTrain)):\n",
    "        tempIn, tempOut = applyAug(inputTrain[i], outputTrain[i])\n",
    "        inputArray = inputArray + tempIn\n",
    "        outputArray = outputArray + tempOut\n",
    "    return randomizeOrder(inputArray, outputArray, len(inputArray))\n",
    "\n",
    "def createValidationSet():\n",
    "    #Passes the entire data set to Augmentation function, structures the entire augmented data and \n",
    "    #randomizes the order using the randomizeOrder function\n",
    "    inputArray = []\n",
    "    outputArray = []\n",
    "    for i in range(len(inputVal)):\n",
    "        tempIn, tempOut = applyAug(inputVal[i], outputVal[i])\n",
    "        inputArray = inputArray + tempIn\n",
    "        outputArray = outputArray + tempOut\n",
    "    return randomizeOrder(inputArray, outputArray, len(inputArray))\n",
    "\n",
    "augIn, augOut = createTrainingSet()\n",
    "\n",
    "augInVal, augOutVal = createValidationSet()\n",
    "\n",
    "def getTrainingSample():\n",
    "    #wrapper to give data to the model in the desired API\n",
    "    return [augIn, augOut]\n",
    "\n",
    "def getValidationSample():\n",
    "    #wrapper to give data to the model in the desired API\n",
    "    return [augInVal, augOutVal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRN2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This code is adapted from pylearn2.\n",
    "    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n",
    "        if n % 2 == 0:\n",
    "            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n",
    "        super(LRN2D, self).__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.beta = beta\n",
    "        self.n = n\n",
    "\n",
    "    def get_output(self, train):\n",
    "        X = self.get_input(train)\n",
    "        b, ch, r, c = K.shape(X)\n",
    "        half_n = self.n // 2\n",
    "        input_sqr = K.square(X)\n",
    "        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))\n",
    "        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n",
    "                                   input_sqr,\n",
    "                                   extra_channels[:, half_n + ch:, :, :]],\n",
    "                                  axis=1)\n",
    "        scale = self.k\n",
    "        for i in range(self.n):\n",
    "            scale += self.alpha * input_sqr[:, i:i + ch, :, :]\n",
    "        scale = scale ** self.beta\n",
    "        return X / scale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__,\n",
    "                  \"alpha\": self.alpha,\n",
    "                  \"k\": self.k,\n",
    "                  \"beta\": self.beta,\n",
    "                  \"n\": self.n}\n",
    "        base_config = super(LRN2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YresCjt0Tk_U"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQX8GILBTTJA"
   },
   "outputs": [],
   "source": [
    "def build(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(96, (11, 11), strides=(4, 4), padding='same', \n",
    "                                     activation='relu', use_bias = 1, \n",
    "                                     kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                     bias_initializer = 'zeros', input_shape=input_shape))\n",
    "    model.add(LRN2D())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(256, (5, 5),\n",
    "                                    activation='relu', use_bias = 1, \n",
    "                                    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                    bias_initializer = 'ones'))\n",
    "    model.add(LRN2D())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), strides=(1, 1), padding='same', \n",
    "                                     activation='relu', use_bias = 1, \n",
    "                                     kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                     bias_initializer = 'zeros'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(384, (3, 3), strides=(1, 1), padding='same', \n",
    "                                     activation='relu', use_bias = 1, \n",
    "                                     kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                     bias_initializer = 'ones'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same', \n",
    "                                     activation='relu', use_bias = 1, \n",
    "                                     kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                     bias_initializer = 'ones'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(4096, activation='relu', use_bias = 1,\n",
    "                                    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                    bias_initializer = 'ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(4096, activation='relu', use_bias = 1,\n",
    "                                    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                    bias_initializer = 'ones'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax', use_bias = 1,\n",
    "                                    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
    "                                    bias_initializer = 'ones'))\n",
    "    \n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='architecture_imagenet.png')\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOOBM5p1UMi_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIvzn7lXUNEH"
   },
   "outputs": [],
   "source": [
    "def plot_graph(model, N):\n",
    "    \n",
    "    plt.figure(0)\n",
    "    plt.plot(np.arange(0, N), model.history['loss'], label='Training loss')\n",
    "    plt.plot(np.arange(0, N), model.history['val_loss'], label='Testing loss')\n",
    "    plt.title('Training vs Testing Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(np.arange(0, N), model.history['categorical_accuracy'], label='Top 1 Training Accuracy')\n",
    "    plt.plot(np.arange(0, N), model.history['val_categorical_accuracy'], label='Top 1 Testing Accuracy')\n",
    "    plt.title('Top 1 Training vs Testing Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(np.arange(0, N), model.history['TopKCategoricalAccuracy'], label='Top 5 Training Accuracy')\n",
    "    plt.plot(np.arange(0, N), model.history['val_TopKCategoricalAccuracy'], label='Top 5 Testing Accuracy')\n",
    "    plt.title('Top 5 Training vs Testing Acuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y37Ur-dMVuhQ"
   },
   "outputs": [],
   "source": [
    "def model_callbacks(name):\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(name, monitor = 'val_categorical_accuracy',\n",
    "    #                             mode = 'max', save_best_only = True, verbose = 1)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', factor = 0.1,    \n",
    "                                  patience = 5, verbose = 1)\n",
    "    return [reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sovZxwQWVvKH",
    "outputId": "6bc2ce5b-f93e-4a34-9d2a-29b89d389f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 7, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 256)         614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 384)         885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         884992    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               819400    \n",
      "=================================================================\n",
      "Total params: 38,130,632\n",
      "Trainable params: 38,129,928\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Train on 400000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "  8300/400000 [..............................] - ETA: 2:02:00 - loss: 5.5266 - categorical_accuracy: 0.0047 - TopKCategoricalAccuracy: 0.0242WARNING:tensorflow:Can save best model only with val_categorical_accuracy available, skipping.\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,TopKCategoricalAccuracy,lr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d43f00396c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m history = model_tinynet.fit(inp, out, batch_size = batch_size, validation_data = (inp_test, out_test), \n\u001b[1;32m     24\u001b[0m                                       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                       callbacks = model_callbacks('model_tiny.h5'))\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all the parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "total_epochs = 60\n",
    "batch_size = 1000\n",
    "input_shape = (56, 56, 3)\n",
    "num_classes = 200\n",
    "\n",
    "inp = np.array(getTrainingSample()[0])\n",
    "out = np.array(getTrainingSample()[1])\n",
    "inp_test = np.array(getValidationSample()[0])\n",
    "out_test = np.array(getValidationSample()[1])\n",
    "\n",
    "# Training TinyNet\n",
    "model_tinynet = build(input_shape, num_classes)\n",
    "model_tinynet.compile(optimizer = tfa.optimizers.weight_decay_optimizers.SGDW(\n",
    "                        learning_rate=learning_rate, momentum=momentum, \n",
    "                        weight_decay=weight_decay, nesterov=True, name='SGDW'),\n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['categorical_accuracy', 'TopKCategoricalAccuracy'])\n",
    "\n",
    "history = model_tinynet.fit(inp, out, batch_size = batch_size, validation_data = (inp_test, out_test), \n",
    "                                      epochs = total_epochs, verbose=1,\n",
    "                                      callbacks = model_callbacks('a'))\n",
    "\n",
    "plot_graph(history, total_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt0fYSQuVx6R"
   },
   "outputs": [],
   "source": [
    "x = getTrainingSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tEstImiuWEoz",
    "outputId": "ffdcfcde-626b-4495-8828-2c540584801b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gYb1T9VEWIre",
    "outputId": "f193aacc-d12a-4311-d531-5a1acd3691f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "er_9fCylWKLK",
    "outputId": "fdd5c33d-f798-4603-af4c-f86cd923d6f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXCyojIMWMm_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "clean-alexnet-keras",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
