{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "V-76o0n56zEg",
    "outputId": "93ee6114-8d8f-4d1e-b339-7e35370522c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhurwadhwa/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "#tf.enable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFbcdH9J7Wsc"
   },
   "source": [
    "#Augmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8u4x7M3X7Vfm"
   },
   "outputs": [],
   "source": [
    "#THIS BLOCK IS IMPORTANT\n",
    "\n",
    "\n",
    "def importClasses():\n",
    "    classPath = \"./tiny-imagenet-200/wnids.txt\"\n",
    "    return open(classPath).read().splitlines()\n",
    "\n",
    "trainClasses = importClasses()\n",
    "\n",
    "def createDict(trainClasses):\n",
    "    #Mapping from class name to class Number\n",
    "    classDict = {}\n",
    "    for i in range(len(trainClasses)):\n",
    "        classDict[trainClasses[i]] = i\n",
    "    return classDict\n",
    "\n",
    "classNumDict = createDict(trainClasses)\n",
    "\n",
    "def getMiddlePatch(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "    except IOError:\n",
    "        return\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.asarray(img)\n",
    "    return img[3:59, 3:59, :]\n",
    "    \n",
    "\n",
    "def pcaAug(original_image):\n",
    "    #Takes in an image as a np array and returns the pca augmented image as a np array\n",
    "    renorm_image = np.reshape(original_image,(original_image.shape[0]*original_image.shape[1],3))\n",
    "\n",
    "    renorm_image = renorm_image.astype('float32')\n",
    "    renorm_image -= np.mean(renorm_image, axis=0)\n",
    "    renorm_image /= np.std(renorm_image, axis=0)\n",
    "\n",
    "    cov = np.cov(renorm_image, rowvar=False)\n",
    "\n",
    "    lambdas, p = np.linalg.eig(cov)\n",
    "    alphas = np.random.normal(0, 0.1, 3)\n",
    "\n",
    "    delta = np.dot(p, alphas*lambdas)\n",
    "\n",
    "    delta = (delta).astype('int8')\n",
    "\n",
    "    pca_color_image = np.maximum(np.minimum(original_image + delta, 255), 0).astype('uint8')\n",
    "    return pca_color_image\n",
    "\n",
    "\n",
    "def transToOneHot(numClass, dims = 200):\n",
    "    vec = np.zeros(dims)\n",
    "    vec[numClass] = 1\n",
    "    return vec\n",
    "\n",
    "def generateImgs(path, outClassNum):\n",
    "    #Takes in path of Image and generates 5 patches and their mirror images and returns them as a list of np arrays and an array of one hot encoded output\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "    except IOError:\n",
    "        return\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.asarray(img)\n",
    "    arr = [img[0:56, 0:56, :], img[8:, 0:56, :], img[0:56, 8:, :], img[8:, 8:, :], img[3:59, 3:59, :]]\n",
    "    arr = arr + [np.flip(arr[0], 1),np.flip(arr[1], 1), np.flip(arr[2], 1), np.flip(arr[3], 1), np.flip(arr[4], 1)]\n",
    "    pcaArr = []\n",
    "    for i in range(len(arr)):\n",
    "        pcaArr.append(pcaAug(arr[i]))\n",
    "    inputArr = arr + pcaArr\n",
    "    oneHot = transToOneHot(outClassNum, dims = 200)\n",
    "    outArr = []\n",
    "    for i in range(len(inputArr)):\n",
    "        outArr.append(oneHot)\n",
    "    return inputArr, outArr\n",
    "\n",
    "def getTrainingSample():\n",
    "    #Returns one batch of training samples using any 5 random images. Returns a batch of 100\n",
    "    inputArray = []\n",
    "    outputArray = []\n",
    "    for i in range(200):\n",
    "        outClassNum = np.random.randint(len(trainClasses))\n",
    "        path = \"./tiny-imagenet-200/train/\" + str(trainClasses[outClassNum]) + \"/images/\" + str(trainClasses[outClassNum]) + \"_\" + str(np.random.randint(100)) + \".JPEG\"\n",
    "        inArr, outArr = generateImgs(path, outClassNum)\n",
    "        inputArray = inputArray + inArr\n",
    "        outputArray = outputArray + outArr\n",
    "    #temp = np.array(100,3)\n",
    "    #temp[:, 0] = inputArray\n",
    "    #temp[:,1] = outputArray\n",
    "    #temp[:,2] = range(100)\n",
    "    #np.random.shuffle(temp)\n",
    "    #inputArray = temp[:,0]\n",
    "    #outputArray = temp[:,1]\n",
    "    return inputArray, outputArray\n",
    "\n",
    "def getOutputArray():\n",
    "    outputArray = []\n",
    "    outputTextPath = \"./tiny-imagenet-200/val/val_annotations.txt\"\n",
    "    textFile = open(outputTextPath).read().split()\n",
    "    for i in range(1000):\n",
    "        outputArray.append(transToOneHot(classNumDict[textFile[6*i+1]], dims = 200))\n",
    "    return outputArray\n",
    "\n",
    "def getTestSample():\n",
    "    inputArray = []\n",
    "    outputArray = getOutputArray()\n",
    "    \n",
    "    \n",
    "    for i in range(1000):\n",
    "        path = \"./tiny-imagenet-200/val/images/val_\" + str(i) + \".JPEG\"\n",
    "        img = getMiddlePatch(path)\n",
    "        inputArray.append(img)\n",
    "        \n",
    "    return inputArray, outputArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#imagenet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRSCtQgU7u1a"
   },
   "outputs": [],
   "source": [
    "image_size = 56 #We should use 56 instead, using 64x64 images\n",
    "\n",
    "input_images = tf.placeholder(tf.float32, shape= [None, image_size, image_size,3], name = \"input_images\")\n",
    "kernel = tf.Variable(tf.truncated_normal([11,11,3,96], dtype=tf.float32, stddev=1e-2), name=\"conv1_weights\")\n",
    "\n",
    "conv = tf.nn.conv2d(input_images, kernel, [1,4,4,1], padding=\"SAME\") #[1,4,4,1] === [1,stride,stride,1] see documentation\n",
    "bias = tf.Variable(tf.truncated_normal([96]), name=\"conv1_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv1 = tf.nn.relu(conv_with_bias, name=\"conv1\")\n",
    "lrn1 = tf.nn.lrn(conv1, alpha = 1e-4, beta=0.75, depth_radius=5, bias=2.0) #LRN = Local Response Normalisation..Depth Radius is most probably n, need to verify\n",
    "pooled_conv1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84KGW9Rw8AFA"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([5,5,96,256], dtype=tf.float32, stddev=1e-2), name=\"conv2_weights\")\n",
    "conv = tf.nn.conv2d(pooled_conv1, kernel, [1, 4, 4, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([256]), name=\"conv2_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv2 = tf.nn.relu(conv_with_bias, name=\"conv2\")\n",
    "lrn2 = tf.nn.lrn(conv2,\n",
    "                 alpha=1e-4,\n",
    "                 beta=0.75,\n",
    "                 depth_radius=5,\n",
    "                 bias=2.0)\n",
    "\n",
    "pooled_conv2 = tf.nn.max_pool(lrn2,\n",
    "                              ksize=[1, 3, 3, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\",\n",
    "                              name=\"pool2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K6HERaR8CpL"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv3_weights\")\n",
    "conv = tf.nn.conv2d(pooled_conv2, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.truncated_normal([384]), name=\"conv3_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv3 = tf.nn.relu(conv_with_bias, name=\"conv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfam5tmu8FVe"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv4_weights\")\n",
    "conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([384]), name=\"conv4_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv4 = tf.nn.relu(conv_with_bias, name=\"conv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gArLGIGV8NHQ"
   },
   "outputs": [],
   "source": [
    "kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                         dtype=tf.float32,\n",
    "                                         stddev=1e-2),\n",
    "                     name=\"conv5_weights\")\n",
    "conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "bias = tf.Variable(tf.ones([256]), name=\"conv5_bias\")\n",
    "conv_with_bias = tf.nn.bias_add(conv, bias)\n",
    "conv5 = tf.nn.relu(conv_with_bias, name=\"conv5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "ARmWuSum8N0m",
    "outputId": "b22fff7f-bc61-4c15-e874-9ea9e18980de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-3d76e4adc7b8>:2: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /Users/madhurwadhwa/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-3d76e4adc7b8>:6: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "fc_size = 256\n",
    "conv5 = tf.layers.flatten(conv5)\n",
    "weights = tf.Variable(tf.truncated_normal([fc_size, fc_size]), name = \"fc1_weights\")\n",
    "bias = tf.Variable(tf.ones([fc_size]), name=\"fc1_bias\")\n",
    "fc1 = tf.matmul(conv5, weights) + bias\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob = 0.5) #Check this line, might cause problem\n",
    "fc1 = tf.nn.relu(fc1, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufUIpZaO8RLj"
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([fc_size, fc_size]), name=\"fc2_weights\")\n",
    "bias = tf.Variable(tf.ones([fc_size]), name=\"fc2_bias\")\n",
    "fc2 = tf.matmul(fc1, weights) + bias\n",
    "fc2 = tf.nn.dropout(fc2, keep_prob = 0.5) #Check this line, might cause problem\n",
    "fc2 = tf.nn.relu(fc2, name=\"fc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-duKoBWc8WeP"
   },
   "outputs": [],
   "source": [
    "n_classes = 200\n",
    "weights = tf.Variable(tf.zeros([fc_size, n_classes]), name=\"output_weights\")\n",
    "bias = tf.Variable(tf.ones([n_classes]), name=\"output_bias\")\n",
    "out = tf.matmul(fc2, weights) + bias\n",
    "out = tf.nn.softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kk0SpxD18Z8O"
   },
   "source": [
    "#training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "-JmP27Vp8bpf",
    "outputId": "94cafb20-05c3-43b2-f80d-be29fa7c0eb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhurwadhwa/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Top 1 Acc (training): 0.0, Top 5 Acc (training) = 0.026499999687075615, Loss: 5.306883335113525 \n",
      "Top 1 Acc (testing): 0.007000000216066837, Top 5 Acc (testing) = 0.019999999552965164, Validation Loss: 5.29987907409668\n",
      "Epoch: 1, Top 1 Acc (training): 0.004999999888241291, Top 5 Acc (training) = 0.021250000223517418, Loss: 5.301883220672607 \n",
      "Top 1 Acc (testing): 0.007000000216066837, Top 5 Acc (testing) = 0.019999999552965164, Validation Loss: 5.29987907409668\n",
      "Epoch: 2, Top 1 Acc (training): 0.0, Top 5 Acc (training) = 0.039000000804662704, Loss: 5.306883335113525 \n",
      "Top 1 Acc (testing): 0.007000000216066837, Top 5 Acc (testing) = 0.02199999988079071, Validation Loss: 5.29987907409668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0671de8040e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         feed_dict={\n\u001b[1;32m     54\u001b[0m                             \u001b[0minput_images\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                             y: out_train})\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# calculating accuracy and loss after 1 epoc for the last training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 960\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1183\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1361\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1350\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1352\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1444\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all the parameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "no_of_epochs = 50\n",
    "total_batches = 5000\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "# cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = out))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) #This Works\n",
    "\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(cost, var_list = var_list, grad_loss=None, name=None) #Testing this\n",
    "\n",
    "# optimizer = tfa.optimizers.weight_decay_optimizers.SGDW(\n",
    "#                 learning_rate=learning_rate, \n",
    "#                 momentum=momentum, \n",
    "#                 weight_decay=weight_decay, \n",
    "#                 nesterov=True, name='SGDW').minimize(cost, var_list = var_list)\n",
    "\n",
    "\n",
    "# accuracy functions\n",
    "top_1 = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "top_1_accuracy = tf.reduce_mean(tf.cast(top_1, tf.float32))\n",
    "top_5 = tf.math.in_top_k(out, tf.argmax(y, 1), 5)\n",
    "top_5_accuracy = tf.reduce_mean(tf.cast(top_5, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # for plotting\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy_top_1 = []\n",
    "    train_accuracy_top_5 = []\n",
    "    test_accuracy_top_1 = []\n",
    "    test_accuracy_top_5 = []\n",
    "\n",
    "    inp_test, out_test = getTestSample() # so that after every epoch we test the same dataset\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "\n",
    "    for epoch in range(no_of_epochs):\n",
    "        # running the architecture for 1 epoc\n",
    "        for batch_no in range(total_batches):\n",
    "            inp_train, out_train = getTrainingSample()\n",
    "            sess.run([optimizer],\n",
    "                        feed_dict={\n",
    "                            input_images: inp_train,\n",
    "                            y: out_train})\n",
    "        \n",
    "        # calculating accuracy and loss after 1 epoc for the last training data\n",
    "        top_1_acc, top_5_acc, loss = sess.run([top_1_accuracy, top_5_accuracy, cost],\n",
    "                                                                feed_dict={\n",
    "                                                                    input_images: inp_train,\n",
    "                                                                    y: out_train})\n",
    "        print(\"Epoch: {}, Top 1 Acc (training): {}, Top 5 Acc (training) = {}, Loss: {} \".format(epoch, top_1_acc, top_5_acc, loss))\n",
    "            \n",
    "        # calculating accuracy and loss after 1 epoc for the test data\n",
    "        test_top_1_acc, test_top_5_acc, valid_loss = sess.run([top_1_accuracy, top_5_accuracy, cost],\n",
    "                                                                feed_dict={\n",
    "                                                                    input_images: inp_test,\n",
    "                                                                    y: out_test})\n",
    "        print(\"Top 1 Acc (testing): {}, Top 5 Acc (testing) = {}, Validation Loss: {}\".format(test_top_1_acc, test_top_5_acc, valid_loss))\n",
    "\n",
    "        # storing for plotting\n",
    "        train_loss.append(loss)\n",
    "        test_loss.append(valid_loss)\n",
    "        train_accuracy_top_1.append(top_1_acc)\n",
    "        train_accuracy_top_5.append(top_5_acc)\n",
    "        test_accuracy_top_1.append(test_top_1_acc)\n",
    "        test_accuracy_top_5.append(test_top_5_acc)\n",
    "\n",
    "    summary_writer.close()\n",
    "\n",
    "    # plotting training and testing - loss\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
    "    plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
    "    plt.title('Training and Test loss')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "    # plotting training and testing - top 1 accuracy\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(len(train_loss)), train_accuracy_top_1, 'b', label='Top 1 Training Accuracy')\n",
    "    plt.plot(range(len(train_loss)), test_accuracy_top_1, 'r', label='Top 1 Test Accuracy')\n",
    "    plt.title('Top 1 Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "    # plotting training and testing - top 5 accuracy\n",
    "    plt.figure(2)\n",
    "    plt.plot(range(len(train_loss)), train_accuracy_top_5, 'b', label='Top 5 Training Accuracy')\n",
    "    plt.plot(range(len(train_loss)), test_accuracy_top_5, 'r', label='Top 5 Test Accuracy')\n",
    "    plt.title('Top 5 Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miUGr7rfQgWT"
   },
   "outputs": [],
   "source": [
    "# Delete Everything after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "alexnet-nnfl.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
